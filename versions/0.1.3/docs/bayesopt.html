<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Bayesian Optimization · Ax</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="In complex engineering problems we often come across parameters that have to be tuned using several time-consuming and noisy evaluations. When the number of parameters is not small or if some of the parameters are continuous, using large factorial designs (e.g., “grid search”) or global optimization techniques for optimization require too many evaluations than is practically feasible. These types of problems show up in a diversity of applications, such as"/><meta name="docsearch:version" content="0.1.3"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Bayesian Optimization · Ax"/><meta property="og:type" content="website"/><meta property="og:url" content="https://ax.dev//versions/0.1.3/index.html"/><meta property="og:description" content="In complex engineering problems we often come across parameters that have to be tuned using several time-consuming and noisy evaluations. When the number of parameters is not small or if some of the parameters are continuous, using large factorial designs (e.g., “grid search”) or global optimization techniques for optimization require too many evaluations than is practically feasible. These types of problems show up in a diversity of applications, such as"/><meta property="og:image" content="https://ax.dev//versions/0.1.3/img/ax.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://ax.dev//versions/0.1.3/img/ax.svg"/><link rel="shortcut icon" href="/versions/0.1.3/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-139570076-1', 'auto');
              ga('send', 'pageview');
            </script><script type="text/javascript" src="https://cdn.plot.ly/plotly-latest.min.js"></script><script type="text/javascript" src="/versions/0.1.3/js/plotUtils.js"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/versions/0.1.3/js/mathjax.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script><script src="https://unpkg.com/vanilla-back-to-top@7.1.14/dist/vanilla-back-to-top.min.js"></script><script>
        document.addEventListener('DOMContentLoaded', function() {
          addBackToTop(
            {"zIndex":100}
          )
        });
        </script><script src="/versions/0.1.3/js/scrollSpy.js"></script><link rel="stylesheet" href="/versions/0.1.3/css/main.css"/><script src="/versions/0.1.3/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/versions/0.1.3/"><img class="logo" src="/versions/0.1.3/img/ax_lockup_white.svg" alt="Ax"/><h2 class="headerTitleWithLogo">Ax</h2></a><a href="/versions/0.1.3/versions.html"><h3>0.1.3</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/versions/0.1.3/docs/why-ax.html" target="_self">Docs</a></li><li class=""><a href="/versions/0.1.3/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/versions/0.1.3/api/" target="_self">API</a></li><li class=""><a href="https://github.com/facebook/Ax" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Algorithms</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Introduction</h3><ul class=""><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/why-ax.html">Why Ax?</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Getting Started</h3><ul class=""><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/installation.html">Installation</a></li><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/api.html">APIs</a></li><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/glossary.html">Glossary</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Algorithms</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/versions/0.1.3/docs/bayesopt.html">Bayesian Optimization</a></li><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/banditopt.html">Bandit Optimization</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Components</h3><ul class=""><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/core.html">Core</a></li><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/runner.html">Trial Evaluation</a></li><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/data.html">Data</a></li><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/models.html">Models</a></li><li class="navListItem"><a class="navItem" href="/versions/0.1.3/docs/storage.html">Storage</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle">Bayesian Optimization</h1></header><article><div><span><p>In complex engineering problems we often come across parameters that have to be tuned using several time-consuming and noisy evaluations. When the number of parameters is not small or if some of the parameters are continuous, using large factorial designs (e.g., “grid search”) or global optimization techniques for optimization require too many evaluations than is practically feasible. These types of problems show up in a diversity of applications, such as</p>
<ol>
<li>Tuning Internet service parameters and selection of weights for recommender systems,</li>
<li>Hyperparameter optimization for machine learning,</li>
<li>Finding optimal set of gait parameters for locomotive control in robotics,</li>
<li>Tuning design parameters and rule-of-thumb heuristics for hardware design.</li>
</ol>
<p>Bayesian optimization (BO) allows us to tune parameters in relatively few iterations by building a smooth model from an initial set of parameter configurations (referred to as the &quot;surrogate model&quot;) to predict the outcomes for yet unexplored parameter configurations. This represents an adaptive approach where the observations from previous evaluations are used to decide what parameter configurations to evaluate next. The same strategy can be used to predict the expected gain from all future evaluations and decide on early termination, if the expected benefit is smaller than what is worthwhile for the problem at hand.</p>
<h2><a class="anchor" aria-hidden="true" id="how-does-it-work"></a><a href="#how-does-it-work" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How does it work?</h2>
<p>Parameter tuning is often done with simple strategies like grid search. However, grid search scales very poorly with the number of parameters (dimensionality) and generally does not work well for more than a couple continuous parameters. Alternative global optimization techniques like DIRECT or genetic algorithms are more flexible, but also typically require more evaluations than is feasible, especially in the presence of uncertainty.</p>
<p>Bayesian optimization starts by building a smooth surrogate model of the outcomes or objective using Gaussian processes (GPs) based on the (possibly noisy) observations available from previous rounds of experimentation. See below for more details on how the GP model works. This surrogate model can be used to make predictions at unobserved parameter configurations and quantify the uncertainty around them. The predictions and the uncertainty estimates are combined to derive an acquisition function, that quantifies the value of observing a particular point. We optimize the acquisition function to find the best point to observe, and then after observing the outcomes a new surrogate model is fitted and the process is repeated until convergence. The entire process is adaptive in the sense that the predictions and uncertainty estimates are updated as new observations are made.</p>
<p>The strategy of relying on successive surrogate models to update our knowledge of the objective allows BO to strike a balance between the conflicting goals of exploration (trying out parameter configurations with high uncertainty in their outcomes) and exploitation (converging on configurations that are likely to be good).  As a result, BO is able to find better configurations with fewer evaluations than generally possible with grid search or other global optimization techniques, making it a good choice for applications where a limited number of function evaluations can be made.</p>
<p><img src="/versions/0.1.3/docs/assets/gp_opt.png" alt="Gaussian process model fit to noisy data"></p>
<p>Figure 1 shows a 1D example, where a surrogate model is fitted to five noisy observations using GPs to predict the objective (solid line) and place uncertainty estimates (proportional to the width of the shaded bands) over the entire x-axis, which represents the range of possible parameter values. The model is able to predict the outcome of configurations that have not yet been tested. As intuitively expected, the uncertainty bands are tight in regions that are well-explored and become wider as we move away from them.</p>
<h2><a class="anchor" aria-hidden="true" id="acquisition-functions"></a><a href="#acquisition-functions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acquisition functions</h2>
<p>BoTorch — Ax's optimization engine — supports some of the most commonly used acquisition functions in BO, like expected improvement, probability of improvement, and upper confidence bound. Expected improvement (EI) is a popular acquisition function owing to its good practical performance and an analytic form that is easy to compute. As the name suggests it rewards evaluation of the objective <em>f</em> based on the expected improvement relative to the current best. If x* is the current best parameter configuration and our goal is to maximize <em>f</em>, then EI is defined as</p>
<p>$$ \text{EI}(x) = \mathbb{E}\bigl[\max(y - f_{max}, 0)\bigr] $$</p>
<p>The parameter configuration with the highest EI is selected and evaluated in the next step. Using an acquisition function like EI to sample new points initially promotes quick exploration because its values, like the uncertainty estimates, are higher in unexplored regions. Once the parameter space is adequately explored, EI naturally narrows in on locations where there is a high likelihood of a good objective value.</p>
<p>The above definition of the EI function assumes that the objective function is observed free of noise. In many types of experiments, such as those found in A/B testing and reinforcement learning, the observations are typically noisy. For these cases, BoTorch implements an efficient variant of EI, called Noisy EI, which allow for optimization of highly noisy outcomes, along with any number of constraints (i.e., ensuring that auxiliary outcomes do not increase or decrease too much). Figure 2 shows how an EI acquisition function can be used in a noisy setting to seamlessly transition from exploration to optimization in BO.  For more on Noisy EI, <a href="https://research.fb.com/efficient-tuning-of-online-systems-using-bayesian-optimization/">see our blog post</a>.</p>
<p><img src="/versions/0.1.3/docs/assets/bo_1d_opt.gif" alt="Bayesian Optimization"></p>
<h2><a class="anchor" aria-hidden="true" id="a-closer-look-at-gaussian-processes"></a><a href="#a-closer-look-at-gaussian-processes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A closer look at Gaussian processes</h2>
<p>How exactly do we model the true objective <em>f</em> for making predictions about yet-to-be-explored regions using only a few noisy observations? GPs are a simple and powerful way of imposing assumptions over functions in the form of a probability distribution. The family of functions is characterized by,</p>
<ol>
<li>A <em>mean function</em> that is the average of all functions, and,</li>
<li>A covariance or <em>kernel function</em> that provides an overall template for the look and feel of the individual functions (such as their shape or smoothness) and how much they can vary around the mean function.</li>
</ol>
<p>In most applications of BO, a radial basis function (RBF) or Matern kernel is used because they allow us the flexibility to fit a wide variety of functions in high dimensions. By default, BoTorch uses the Matern 5/2 kernel, which tends to allow for less smooth surfaces, compared to the RBF. For more mathematical details and intuitions about GPs and the different kernels check out <a href="https://distill.pub/2019/visual-exploration-gaussian-processes">this tutorial</a>.</p>
<p>In GP regression, the true objective is specified by a GP prior distribution with mean zero and a kernel function. Given a set of noisy observations from initial experimental evaluations, a Bayesian update gives the posterior distribution which is itself a GP with an updated mean and kernel function. The mean function of the posterior distribution gives the best prediction at any point conditional on the available observations, and the kernel function helps to quantify the uncertainty in the predictions in terms of posterior predictive intervals. Figure 3 shows three draws from the posterior GP as well as the predictions and posterior predictive intervals.</p>
<p><img src="/versions/0.1.3/docs/assets/gp_posterior.png" alt="GP Posterior draws and predictive intervals"></p>
<p>The kernel function has several hyperparameters that determine how smooth the GP posterior will be. For the predictions and uncertainty estimates to be practically useful, we have to make sure that the kernel is adapted to the observations. This is done by fitting the kernel hyperparameters to the data, usually by maximizing the marginal likelihood of the data, or with MCMC.</p>
<p>For detailed information about Ax's underlying Bayesian optimization engine, BoTorch, see the <a href="https://botorch.org/docs/introduction">BoTorch documentation</a>.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/versions/0.1.3/docs/glossary.html"><span class="arrow-prev">← </span><span>Glossary</span></a><a class="docs-next button" href="/versions/0.1.3/docs/banditopt.html"><span>Bandit Optimization</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#how-does-it-work">How does it work?</a></li><li><a href="#acquisition-functions">Acquisition functions</a></li><li><a href="#a-closer-look-at-gaussian-processes">A closer look at Gaussian processes</a></li></ul></nav></div><footer class="nav-footer" id="footer"><a href="https://code.facebook.com/projects/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/versions/0.1.3/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2019 Facebook Inc.</section></footer></div></body></html>