# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

# pyre-strict

import numpy as np
import torch
from ax.core.experiment import Experiment
from ax.core.objective import MultiObjective, ScalarizedObjective
from ax.core.outcome_constraint import ComparisonOp, OutcomeConstraint
from ax.exceptions.core import UnsupportedError, UserInputError
from ax.generation_strategy.generation_strategy import GenerationStrategy
from ax.modelbridge.base import Adapter

from botorch.utils.probability.utils import compute_log_prob_feas_from_bounds
from numpy.typing import NDArray
from pyre_extensions import none_throws

# Because normal distributions have long tails, every arm has a non-zero
# probability of violating the constraint. But below a certain threshold, we
# consider probability of violation to be negligible.
MINIMUM_CONTRAINT_VIOLATION_THRESHOLD = 0.01

# Plotting style constants
CONFIDENCE_INTERVAL_BLUE = "rgba(0, 0, 255, 0.2)"
MARKER_BLUE = "rgba(0, 0, 255, 0.3)"  # slightly more opaque than the CI blue
CANDIDATE_RED = "rgba(220, 20, 60, 0.3)"
CANDIDATE_CI_RED = "rgba(220, 20, 60, 0.2)"


def get_constraint_violated_probabilities(
    predictions: list[tuple[dict[str, float], dict[str, float]]],
    outcome_constraints: list[OutcomeConstraint],
) -> dict[str, list[float]]:
    """Get the probability that each arm violates the outcome constraints.

    Args:
        predictions: List of predictions for each observation feature
            generated by predict_at_point.  It should include predictions
            for all outcome constraint metrics.
        outcome_constraints: List of outcome constraints to check.

    Returns:
        A dict of probabilities that each arm violates the outcome
        constraint provided, and for "any_constraint_violated" the probability that
        the arm violates *any* outcome constraint provided.
    """
    if len(outcome_constraints) == 0:
        return {"any_constraint_violated": [0.0] * len(predictions)}
    if any(constraint.relative for constraint in outcome_constraints):
        raise UserInputError(
            "`get_constraint_violated_probabilities()` does not support relative "
            "outcome constraints. Use `Derelativize().transform_optimization_config()` "
            "before passing constraints to this method."
        )

    metrics = [constraint.metric.name for constraint in outcome_constraints]
    means = torch.as_tensor(
        [
            [prediction[0][metric_name] for metric_name in metrics]
            for prediction in predictions
        ]
    )
    sigmas = torch.as_tensor(
        [
            [prediction[1][metric_name] for metric_name in metrics]
            for prediction in predictions
        ]
    )
    feasibility_probabilities: dict[str, NDArray] = {}
    for constraint in outcome_constraints:
        if constraint.op == ComparisonOp.GEQ:
            con_lower_inds = torch.tensor([metrics.index(constraint.metric.name)])
            con_lower = torch.tensor([constraint.bound])
            con_upper_inds = torch.as_tensor([])
            con_upper = torch.as_tensor([])
        else:
            con_lower_inds = torch.as_tensor([])
            con_lower = torch.as_tensor([])
            con_upper_inds = torch.tensor([metrics.index(constraint.metric.name)])
            con_upper = torch.tensor([constraint.bound])

        feasibility_probabilities[constraint.metric.name] = (
            compute_log_prob_feas_from_bounds(
                means=means,
                sigmas=sigmas,
                con_lower_inds=con_lower_inds,
                con_upper_inds=con_upper_inds,
                con_lower=con_lower,
                con_upper=con_upper,
                # "both" can also be expressed by 2 separate constraints...
                con_both_inds=torch.as_tensor([]),
                con_both=torch.as_tensor([]),
            )
            .exp()
            .numpy()
        )

    feasibility_probabilities["any_constraint_violated"] = np.prod(
        list(feasibility_probabilities.values()), axis=0
    )

    return {
        metric_name: (1 - feasibility_probabilities[metric_name]).tolist()
        for metric_name in feasibility_probabilities
    }


def format_constraint_violated_probabilities(
    constraints_violated: dict[str, float],
) -> str:
    """Format the constraints violated for the tooltip."""
    max_metric_length = 70
    constraints_violated = {
        k: v
        for k, v in constraints_violated.items()
        if v > MINIMUM_CONTRAINT_VIOLATION_THRESHOLD
    }
    constraints_violated_str = "<br />  ".join(
        [
            (
                f"{k[:max_metric_length]}{'...' if len(k) > max_metric_length else ''}"
                f": {v * 100:.1f}% chance violated"
            )
            for k, v in constraints_violated.items()
        ]
    )
    if len(constraints_violated_str) == 0:
        return "No constraints violated"
    else:
        constraints_violated_str = "<br />  " + constraints_violated_str

    return constraints_violated_str


def get_nudge_value(
    metric_name: str,
    experiment: Experiment | None = None,
    use_modeled_effects: bool = False,
) -> int:
    """Get the amount to nudge the level of the plot. Deteremined by metric
    importance and whether modeled effects are used.
    """
    # without an experiment or optimization config, we can't tell if this plot is
    # relatively more important
    if experiment is None or experiment.optimization_config is None:
        return 0

    nudge = 0
    # More important metrics have a higher nudge
    if metric_name in experiment.optimization_config.objective.metric_names:
        nudge += 2
    elif metric_name in experiment.optimization_config.metrics:
        nudge += 1

    # Relevant for plots where observed effects and modeled effects can both be shown
    if use_modeled_effects:
        nudge += 1

    return nudge


def get_adapter(
    analysis_name: str,
    experiment: Experiment | None = None,
    generation_strategy: GenerationStrategy | None = None,
    adapter: Adapter | None = None,
    enforce_supports_predictions: bool = False,
) -> Adapter:
    """
    Select the appropriate adapter for the analysis being performed.

    Args:
        analysis_name: The name of the analysis card for error logging
        experiment: The experiment associated with this analysis
        generation_strategy: The generation strategy associated with this analysis
        adapter: A custom adapter that can be passed in during adhoc computation. This
            will always take precedence.
    """
    # If adapter is provided, it will take precedence, otherwise use the current
    # adapter from the generation strategy
    if adapter is None:
        if generation_strategy is None:
            raise UserInputError(
                f"{analysis_name} requires a GenerationStrategy if no custom "
                "adapter is provided."
            )

        # If model is not fit already, fit it
        if generation_strategy.model is None:
            if experiment is None:
                raise UserInputError(
                    "Unable to find a model on the GenerationStrategy,"
                    " so Experiment must be provided to fit the model"
                    f" to compute {analysis_name}."
                )
            generation_strategy._curr._fit(experiment=experiment)
        adapter = none_throws(generation_strategy.model)  # model should be fit now

    # if the adapter requested must support predictions, but does not, raise an error
    if enforce_supports_predictions and not is_predictive(adapter=adapter):
        raise UserInputError(
            f"{analysis_name} requires a GenerationStrategy which is "
            "in a state where the current model supports prediction. "
            f"The current model is {adapter._model_key} and does not support "
            "prediction."
        )

    return adapter


def is_predictive(adapter: Adapter) -> bool:
    # TODO: Improve this logic and move it to base adapter class
    """Check if a adapter is predictive.  Basically, we're checking if
    predict() is implemented.

    NOTE: This does not mean it's capable of out of sample prediction.
    """
    try:
        adapter.predict(observation_features=[])
    except NotImplementedError:
        return False
    except Exception:
        return True
    return True


def select_metric(experiment: Experiment) -> str:
    """Select the most relevant metric to plot from an Experiment."""
    if experiment.optimization_config is None:
        raise ValueError(
            "Cannot infer metric to plot from Experiment without OptimizationConfig"
        )
    objective = experiment.optimization_config.objective
    if isinstance(objective, MultiObjective):
        raise UnsupportedError(
            "Cannot infer metric to plot from MultiObjective, please "
            "specify a metric"
        )
    if isinstance(objective, ScalarizedObjective):
        raise UnsupportedError(
            "Cannot infer metric to plot from ScalarizedObjective, please "
            "specify a metric"
        )
    return experiment.optimization_config.objective.metric.name


def get_autoset_axis_limits(
    y: np.typing.NDArray,
    optimization_direction: str,
    force_include_value: float | None = None,
) -> list[float]:
    """Provides automatic axis limits based on the data and optimization direction.
    All best points are included in this range, and by default the worst points are
    truncated at some distance below the median, where that distance is given by
    1.5 * (the distance between the median and the best quartile).

    If `force_include_value` is provided, the worst points will be truncated at this
    value if it is worse than the truncation point described above.
    """
    q1 = np.percentile(y, q=25, method="lower").min()
    q2_min = np.percentile(y, q=50, method="linear").min()
    q2_max = np.percentile(y, q=50, method="linear").max()
    q3 = np.percentile(y, q=75, method="higher").max()
    if optimization_direction == "minimize":
        y_lower = y.min()
        y_upper = q2_max + 1.5 * (q2_max - q1)
        if force_include_value is not None:
            y_upper = max(y_upper, force_include_value)
    else:
        y_lower = q2_min - 1.5 * (q3 - q2_min)
        y_upper = y.max()
        if force_include_value is not None:
            y_lower = min(y_lower, force_include_value)
    y_padding = 0.1 * (y_upper - y_lower)
    y_lower, y_upper = y_lower - y_padding, y_upper + y_padding
    return [y_lower, y_upper]
