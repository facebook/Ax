{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "f2c99ee2-a85b-4cad-a5ff-1e2976bbc306",
    "showInput": false
   },
   "source": [
    "# Fully Bayesian Multi-Objective Optimization using qNEHVI + SAASBO\n",
    "\n",
    "### This Tutorial\n",
    "\n",
    "This tutorial will show how to use qNEHVI with fully bayesian inference for multi-objective \n",
    "optimization.\n",
    "\n",
    "Multi-objective optimization (MOO) covers the case where we care about multiple\n",
    "outcomes in our experiment but we do not know before hand a specific weighting of those\n",
    "objectives (covered by `ScalarizedObjective`) or a specific constraint on one objective \n",
    "(covered by `OutcomeConstraint`s) that will produce the best result.\n",
    "\n",
    "The solution in this case is to find a whole Pareto frontier, a surface in outcome-space\n",
    "containing points that can't be improved on in every outcome. This shows us the\n",
    "tradeoffs between objectives that we can choose to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "originalKey": "0aaae64b-d420-45d5-9597-52c09429d562",
    "showInput": true
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "Optimize a list of M objective functions $ \\bigl(f^{(1)}( x),..., f^{(M)}( x) \\bigr)$ over a bounded search space $\\mathcal X \\subset \\mathbb R^d$.\n",
    "\n",
    "We assume $f^{(i)}$ are expensive-to-evaluate black-box functions with no known analytical expression, and no observed gradients. For instance, a machine learning model where we're interested in maximizing accuracy and minimizing inference time, with $\\mathcal X$ the set of possible configuration spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "470d5165-7f9d-4fbc-99fd-39d1015c7be0",
    "showInput": false
   },
   "source": [
    "### Fully Bayesian Inference\n",
    "\n",
    "Previous work, has shown that using a fully Bayesian treatment of GP model hyperparameters $\\boldsymbol \\theta$ can lead to improved closed loop Bayesian optimization performance [1]. Snoek et al [1] propose to use an integrated acquisition function $\\alpha_{MCMC}$ where the base acquisition function $\\alpha(\\mathbf{x} | \\boldsymbol \\theta, \\mathcal D)$ is integrated over the the posterior distribution over the hyperparameters $p({\\boldsymbol{\\theta}} | \\mathcal{D})$, where $ \\mathcal{D} = \\{{\\mathbf{x}}_i, y_i\\}_{i=1}^n$:\n",
    "\n",
    "$\\alpha_{MCMC}(\\mathbf{x}, \\mathcal D) = \\int \\alpha(\\mathbf{x} | \\boldsymbol \\theta, \\mathcal D) p(\\boldsymbol \\theta | \\mathcal D) d\\boldsymbol \\theta$\n",
    "\n",
    "\n",
    "Since  $p({\\boldsymbol{\\theta}} | \\mathcal{D})$ typically cannot be expressed in closed-form, Markov Chain Monte-Carlo (MCMC) methods are used to draw samples from $p({\\boldsymbol{\\theta}} | \\mathcal{D})$. In this tutorial we use the NUTS sampler from the pyro package for automatic, robust fully Bayesian inference.\n",
    "\n",
    "[1] J. Snoek, H. Larochelle, R. P. Adams, Practical Bayesian Optimization of Machine Learning Algorithms. Advances in Neural Information Processing Systems 26, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAAS Priors (SAASBO)\n",
    "\n",
    "Recently Eriksson et al [2] propose using sparse axis-aligned subspace priors for Bayesian optimization over high-dimensional search spaces. Specifically, the authors propose using a hierarchical sparsity prior consisting of a global shrinkage parameter with a Half-Cauchy prior $\\tau \\sim \\mathcal{HC}(\\beta)$, and ARD lengthscales $\\rho_d \\sim \\mathcal{HC}(\\tau)$ for $d=1, ..., D$. See [2] for details. \n",
    "\n",
    "[2] D. Eriksson, M. Jankowiak. High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces. Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "213ff269-6109-408a-89b3-e92393e3c31f",
    "showInput": false
   },
   "source": [
    "### qNEHVI \n",
    "\n",
    "In this tutorial, we use qNEHVI [3] as our acquisition function for multi-objective optimization. We integrate qNEHVI over the posterior distribution of the GP hyperparameters as proposed in [4].\n",
    "\n",
    "[3] S. Daulton, M. Balandat, E. Bakshy. Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Arxiv, 2021.\n",
    "\n",
    "[4] D. Eriksson, P. Chuang, S. Daulton, P. Xia, A. Shrivastava, A. Babu, S. Zhao, A. Aly, G. Venkatesh, M. Balandat. Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization. ICML AutoML Workshop, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "originalKey": "47e79bce-564d-40a6-84a6-0003ebdda93d"
   },
   "source": [
    "### Further Information\n",
    "\n",
    "For a deeper explanation of multi-objective optimization, please refer to the dedicated multi-objective optimization tutorial: https://ax.dev/tutorials/multiobjective_optimization.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "dabdd6f6-34b3-4103-b599-bc909fe9faab"
   },
   "source": [
    "## Setup\n",
    "\n",
    "In this tutorial, we use Ax Developer API. Additional resources:\n",
    "- To learn more about the developer API, refer to the dedicated tutorial: https://ax.dev/tutorials/gpei_hartmann_developer.html. \n",
    "- To set up a `GenerationStrategy` with multi-objective SAASBO (and use it in Ax Service API), follow the generation strategy tutorial: https://ax.dev/tutorials/generation_strategy.html and use `Models.FULLYBAYESIANMOO` for the Bayesian optimization generation step.\n",
    "- To learn about multi-objective optimization in Ax Service API: https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "03b8cd70-54f4-4d4d-8445-60439ba00e27"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ax import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ax.metrics.noisy_function import GenericNoisyFunctionMetric\n",
    "from ax.service.utils.report_utils import exp_to_df\n",
    "from ax.runners.synthetic import SyntheticRunner\n",
    "\n",
    "# Plotting imports and initialization\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.pareto_utils import compute_posterior_pareto_frontier\n",
    "from ax.plot.pareto_frontier import plot_pareto_frontier\n",
    "\n",
    "init_notebook_plotting()\n",
    "\n",
    "# Model registry for creating multi-objective optimization models.\n",
    "from ax.modelbridge.registry import Models\n",
    "\n",
    "# Analysis utilities, including a method to evaluate hypervolumes\n",
    "from ax.modelbridge.modelbridge_utils import observed_hypervolume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "originalKey": "122d77fa-21b8-4b01-9522-eae5990aba86"
   },
   "source": [
    "### Load our sample 2-objective problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "744782ab-028f-4bbf-ba0a-eec8520c2fcf"
   },
   "outputs": [],
   "source": [
    "from botorch.test_functions.multi_objective import DTLZ2\n",
    "d = 10\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double, \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "problem = DTLZ2(num_objectives=2, dim=d, negate=True).to(**tkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "originalKey": "02a84443-ed1c-4e63-b2f8-9f1a77d530ec"
   },
   "source": [
    "## Define experiment configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "5dd66dc9-86a3-44a0-8109-418de66edfdb"
   },
   "source": [
    "### Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "6060bdaf-be41-4d1d-9407-463a1e0c17f3"
   },
   "outputs": [],
   "source": [
    "search_space = SearchSpace(\n",
    "    parameters=[RangeParameter(name=f\"x{i}\", lower=0, upper=1, parameter_type=ParameterType.FLOAT) for i in range(d)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "4d5ffaaa-6aca-4502-9aac-047806c4a550",
    "showInput": false
   },
   "source": [
    "### MultiObjectiveOptimizationConfig\n",
    "\n",
    "To optimize multiple objective we must create a `MultiObjective` containing the metrics we'll optimize and `MultiObjectiveOptimizationConfig` (which contains `ObjectiveThreshold`s) instead of our more typical `Objective` and `OptimizationConfig`. Additional resources:\n",
    "- To set up a custom metric for your problem, refer to the dedicated section of the Developer API tutorial: https://ax.dev/tutorials/gpei_hartmann_developer.html#8.-Defining-custom-metrics.\n",
    "- To avoid needing to setup up custom metrics by using multi-objective optimization in Ax Service API: https://ax.dev/tutorials/multiobjective_optimization.html#Using-the-Service-API.\n",
    "\n",
    "We define `GenericNoisyFunctionMetric`s to wrap our synthetic Branin-Currin problem's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [f\"x{i}\" for i in range(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "fbf29141-2d4b-4dc9-aca7-e13e93369c36"
   },
   "outputs": [],
   "source": [
    "def f1(x) -> float:\n",
    "        x_sorted = [x[p_name] for p_name in param_names]\n",
    "        return float(problem(torch.tensor(x_sorted, **tkwargs).clamp(0., 1.))[0])\n",
    "    \n",
    "def f2(x) -> float:\n",
    "        x_sorted = [x[p_name] for p_name in param_names]\n",
    "        return float(problem(torch.tensor(x_sorted, **tkwargs).clamp(0., 1.))[1])\n",
    "\n",
    "metric_a = GenericNoisyFunctionMetric(\"a\", f=f1, noise_sd=0.0, lower_is_better=False)\n",
    "metric_b = GenericNoisyFunctionMetric(\"b\", f=f2, noise_sd=0.0, lower_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "a248dc3d-d053-439c-a4ff-c226105a0bfb"
   },
   "outputs": [],
   "source": [
    "mo = MultiObjective(\n",
    "    objectives=[Objective(metric=metric_a), Objective(metric=metric_b)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "originalKey": "cefa9d16-a23a-4222-82fb-e33ce89ddb58"
   },
   "outputs": [],
   "source": [
    "objective_thresholds = [\n",
    "    ObjectiveThreshold(metric=metric, bound=val, relative=False)\n",
    "    for metric, val in zip(mo.metrics, problem.ref_point)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "2512e114-8693-4ea1-8938-db0899a4f929"
   },
   "outputs": [],
   "source": [
    "optimization_config = MultiObjectiveOptimizationConfig(\n",
    "    objective=mo,\n",
    "    objective_thresholds=objective_thresholds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "b689c7a9-28f8-47ae-a5da-c3a93674e72d",
    "showInput": false
   },
   "source": [
    "## Define experiment creation utilities\n",
    "\n",
    "These construct our experiment, then initialize with Sobol points before we fit a Gaussian Process model to those initial points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "originalKey": "fb09ef7d-e744-472b-9290-ec24eb40d3fe"
   },
   "outputs": [],
   "source": [
    "N_INIT = 2 * (d + 1)\n",
    "N_BATCH = 10\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "b9b934cb-3afe-4a39-812b-c4d3bca194b6"
   },
   "outputs": [],
   "source": [
    "def build_experiment():\n",
    "    experiment = Experiment(\n",
    "        name=\"pareto_experiment\",\n",
    "        search_space=search_space,\n",
    "        optimization_config=optimization_config,\n",
    "        runner=SyntheticRunner(),\n",
    "    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "cf05b5ca-ee87-45be-a028-51952fb4a2ee"
   },
   "outputs": [],
   "source": [
    "## Initialize with Sobol samples\n",
    "\n",
    "def initialize_experiment(experiment):\n",
    "    sobol = Models.SOBOL(search_space=experiment.search_space)\n",
    "\n",
    "    experiment.new_batch_trial(sobol.gen(N_INIT)).run()\n",
    "\n",
    "    return experiment.fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "96a350f9-5fa1-45a9-aac2-42d942e939f6"
   },
   "source": [
    "## qNEHVI + SAASBO\n",
    "Noisy expected hypervolume improvement + fully Bayesian inference with SAAS priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "02a0d667-9e8e-43b9-b2ef-09ff2b2d85ba"
   },
   "outputs": [],
   "source": [
    "experiment = build_experiment()\n",
    "data = initialize_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "8ec2a5a3-bb79-435d-834c-55510ec52b15"
   },
   "outputs": [],
   "source": [
    "from ax.core.metric import Metric\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import DominatedPartitioning\n",
    "\n",
    "hv_list = []\n",
    "model = None\n",
    "for i in range(N_BATCH):   \n",
    "    model = Models.FULLYBAYESIANMOO(\n",
    "        experiment=experiment, \n",
    "        data=data,\n",
    "        # use fewer num_samples and warmup_steps to speed up this tutorial\n",
    "        num_samples=256,\n",
    "        warmup_steps=512,\n",
    "        torch_device=tkwargs[\"device\"],\n",
    "        verbose=False,  # Set to True to print stats from MCMC\n",
    "        disable_progbar=True,  # Set to False to print a progress bar from MCMC\n",
    "    )\n",
    "    generator_run = model.gen(BATCH_SIZE)\n",
    "    trial = experiment.new_batch_trial(generator_run=generator_run)\n",
    "    trial.run()\n",
    "    data = Data.from_multiple_data([data, trial.fetch_data()])\n",
    "    \n",
    "    exp_df = exp_to_df(experiment)\n",
    "    outcomes = torch.tensor(exp_df[['a', 'b']].values, **tkwargs)\n",
    "    partitioning = DominatedPartitioning(ref_point=problem.ref_point, Y=outcomes)\n",
    "    try:\n",
    "        hv = partitioning.compute_hypervolume().item()\n",
    "    except:\n",
    "        hv = 0\n",
    "        print(\"Failed to compute hv\")\n",
    "    hv_list.append(hv)\n",
    "    print(f\"Iteration: {i}, HV: {hv}\")\n",
    "\n",
    "df = exp_to_df(experiment).sort_values(by=[\"trial_index\"])\n",
    "outcomes = df[[\"a\", \"b\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden_ranges": [],
    "originalKey": "bafe189b-88cb-4a9e-aeff-2d2945d497da"
   },
   "source": [
    "## Plot empirical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden_ranges": [],
    "originalKey": "5cc39663-a778-4600-bf39-57e63a7c2f39",
    "showInput": false
   },
   "source": [
    "#### Plot observed hypervolume, with color representing the iteration that a point was generated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "94ba246d-6adb-42bc-8f24-c10266b165d8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.cm import ScalarMappable\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8,6))\n",
    "algos = [\"qNEHVI\"]\n",
    "train_obj = outcomes\n",
    "cm = plt.cm.get_cmap('viridis')\n",
    "\n",
    "n_results = N_BATCH*BATCH_SIZE + N_INIT\n",
    "\n",
    "batch_number = df.trial_index.values\n",
    "sc = axes.scatter(train_obj[:, 0], train_obj[:,1], c=batch_number, alpha=0.8)\n",
    "axes.set_title(algos[0])\n",
    "axes.set_xlabel(\"Objective 1\")\n",
    "axes.set_ylabel(\"Objective 2\")\n",
    "norm = plt.Normalize(batch_number.min(), batch_number.max())\n",
    "sm =  ScalarMappable(norm=norm, cmap=cm)\n",
    "sm.set_array([])\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.ax.set_title(\"Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "87e98991-aa2d-497b-925c-ee4cc82cf2f9"
   },
   "source": [
    "# Hypervolume statistics\n",
    "The hypervolume of the space dominated by points that dominate the reference point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "2401a7cb-e825-489a-994f-c252050310f3"
   },
   "source": [
    "#### Plot the results\n",
    "The plot below shows a common metric of multi-objective optimization performance when the true Pareto frontier is known:  the log difference between the hypervolume of the true Pareto front and the hypervolume of the approximate Pareto front identified by qNEHVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden_ranges": [],
    "originalKey": "05bf3b39-9cce-4a58-bc22-ed6a59a8c531"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "iters = np.arange(1, N_BATCH + 1)\n",
    "log_hv_difference = np.log10(problem.max_hv - np.asarray(hv_list))[:N_BATCH + 1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.plot(iters, log_hv_difference, label=\"qNEHVI+SAASBO\", linewidth=1.5)\n",
    "ax.set(xlabel='Batch Iterations', ylabel='Log Hypervolume Difference')\n",
    "ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Model fits\n",
    "\n",
    "Here, we examine the GP model fits using the fully bayesian inference with SAAS priors. We plot the leave-one-out cross-validation below. Note: model hyperparameters are not re-sampled on each fold to reduce the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.modelbridge.cross_validation import cross_validate\n",
    "from ax.plot.diagnostic import tile_cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cross_validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(tile_cross_validation(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.modelbridge.cross_validation import compute_diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute  out-of-sample log likelihood\n",
    "compute_diagnostics(cv)[\"Log likelihood\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we examine the GP model fits using MAP estimation for comparison. The fully bayesian model has a higher log-likelihood than the MAP model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_model = Models.GPEI(experiment=experiment, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_cv = cross_validate(map_model)\n",
    "render(tile_cross_validation(map_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute out-of-sample log likelihood\n",
    "compute_diagnostics(map_cv)[\"Log likelihood\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
