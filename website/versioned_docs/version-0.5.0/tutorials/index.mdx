---
title: Welcome to Ax Tutorials
sidebar_label: Overview
---

Here you can learn about the structure and applications of Ax from examples.

**Our 3 API tutorials:** [Loop](/tutorials/gpei_hartmann_loop/index.mdx), [Service](/tutorials/gpei_hartmann_service/index.mdx), and [Developer](/tutorials/gpei_hartmann_developer/index.mdx) — are a good place to start. Each tutorial showcases optimization on a constrained Hartmann6 problem, with the Loop API being the simplest to use and the Developer API being the most customizable.

**NOTE: We recommend the [Service API](/tutorials/gpei_hartmann_service/index.mdx) for the vast majority of use cases.** This API provides an ideal balance of flexibility and simplicity for most users, and we are in the process of consolidating Ax usage around it more formally.

**Further, we explore the different components available in Ax in more detail.** {' '} The components explored below serve to set up an experiment, visualize its results, configure an optimization algorithm, run an entire experiment in a managed closed loop, and combine BoTorch components in Ax in a modular way.

*   [Visualizations](/tutorials/visualizations/index.mdx) illustrates the different plots available to view and understand your results.

*   [GenerationStrategy](/tutorials/generation_strategy/index.mdx) steps through setting up a way to specify the optimization algorithm (or multiple). A `GenerationStrategy` is an important component of Service API and the `Scheduler`.

*   [Scheduler](/tutorials/scheduler/index.mdx) demonstrates an example of a managed and configurable closed-loop optimization, conducted in an asyncronous fashion. `Scheduler` is a manager abstraction in Ax that deploys trials, polls them, and uses their results to produce more trials.

*   [Modular `BoTorchModel`](/tutorials/modular_botax/index.mdx) walks though a new beta-feature — an improved interface between Ax and{' '} [BoTorch](https://botorch.org/) — which allows for combining arbitrary BoTorch components like `AcquisitionFunction`, `Model`, `AcquisitionObjective` etc. into a single{' '} `Model` in Ax.

**Our other Bayesian Optimization tutorials include:**

*   [Hyperparameter Optimization for PyTorch](/tutorials/tune_cnn_service/index.mdx) provides an example of hyperparameter optimization with Ax and integration with an external ML library.

*   [Hyperparameter Optimization on SLURM via SubmitIt](/tutorials/submitit/index.mdx) shows how to use the AxClient to schedule jobs and tune hyperparameters on a Slurm cluster.

*   [Multi-Task Modeling](/tutorials/multi_task/index.mdx) illustrates multi-task Bayesian Optimization on a constrained synthetic Hartmann6 problem.

*   [Multi-Objective Optimization](/tutorials/multiobjective_optimization/index.mdx) demonstrates Multi-Objective Bayesian Optimization on a synthetic Branin-Currin test function.

*   [Trial-Level Early Stopping](/tutorials/early_stopping/index.mdx) shows how to use trial-level early stopping on an ML training job to save resources and iterate faster.

{/* *   [Benchmarking Suite](/tutorials/benchmarking_suite_example/index.mdx) demonstrates how to use the Ax benchmarking suite to compare Bayesian Optimization algorithm performances and generate a comparative report with visualizations. */}

For experiments done in a real-life setting, refer to our field experiments tutorials:

*   [Bandit Optimization](/tutorials/factorial/index.mdx) shows how Thompson Sampling can be used to intelligently reallocate resources to well-performing configurations in real-time.

*   [Human-in-the-Loop Optimization](/tutorials/human_in_the_loop/index.mdx) walks through manually influencing the course of optimization in real-time.
