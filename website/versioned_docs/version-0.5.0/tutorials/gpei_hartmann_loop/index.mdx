---
title: Loop API
sidebar_label: Loop API
---

import LinkButtons from "@site/src/components/LinkButtons.jsx";
import CellOutput from "@site/src/components/CellOutput.jsx";
import {PlotlyFigure} from "@site/src/components/Plotting.jsx";

<LinkButtons
  githubUrl="https://github.com/facebook/ax/blob/0.5.0/tutorials/gpei_hartmann_loop/gpei_hartmann_loop.ipynb"
  colabUrl="https://colab.research.google.com/github/facebook/ax/blob/0.5.0/tutorials/gpei_hartmann_loop/gpei_hartmann_loop.ipynb"
/>

# Loop API Example on Hartmann6

The loop API is the most lightweight way to do optimization in Ax. The user makes one
call to `optimize`, which performs all of the optimization under the hood and returns
the optimized parameters.

For more customizability of the optimization procedure, consider the Service or
Developer API.

```python
import sys
in_colab = 'google.colab' in sys.modules
if in_colab:
    %pip install ax-platform
```

```python
import numpy as np
from ax.metrics.branin import branin

from ax.plot.contour import plot_contour
from ax.plot.trace import optimization_trace_single_method
from ax.service.managed_loop import optimize
from ax.utils.measurement.synthetic_functions import hartmann6
from ax.utils.notebook.plotting import init_notebook_plotting, render
import plotly.io as pio

init_notebook_plotting()
if in_colab:
    pio.renderers.default = "colab"
```

<CellOutput>
{
  `[INFO 02-03 18:30:12] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:12] ax.utils.notebook.plotting: Please see
    (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering)
    if visualizations are not rendering.`
}
</CellOutput>

## 1. Define evaluation function

First, we define an evaluation function that is able to compute all the metrics needed
for this experiment. This function needs to accept a set of parameter values and can
also accept a weight. It should produce a dictionary of metric names to tuples of mean
and standard error for those metrics.

```python
def hartmann_evaluation_function(parameterization):
    x = np.array([parameterization.get(f"x{i+1}") for i in range(6)])
    # In our case, standard error is 0, since we are computing a synthetic function.
    return {"hartmann6": (hartmann6(x), 0.0), "l2norm": (np.sqrt((x**2).sum()), 0.0)}
```

If there is only one metric in the experiment – the objective – then evaluation function
can return a single tuple of mean and SEM, in which case Ax will assume that evaluation
corresponds to the objective. It can also return only the mean as a float, in which case
Ax will treat SEM as unknown and use a model that can infer it. For more details on
evaluation function, refer to the "Trial Evaluation" section in the docs.

## 2. Run optimization

The setup for the loop is fully compatible with JSON. The optimization algorithm is
selected based on the properties of the problem search space.

```python
best_parameters, values, experiment, model = optimize(
    parameters=[
        {
            "name": "x1",
            "type": "range",
            "bounds": [0.0, 1.0],
            "value_type": "float",  # Optional, defaults to inference from type of "bounds".
            "log_scale": False,  # Optional, defaults to False.
        },
        {
            "name": "x2",
            "type": "range",
            "bounds": [0.0, 1.0],
        },
        {
            "name": "x3",
            "type": "range",
            "bounds": [0.0, 1.0],
        },
        {
            "name": "x4",
            "type": "range",
            "bounds": [0.0, 1.0],
        },
        {
            "name": "x5",
            "type": "range",
            "bounds": [0.0, 1.0],
        },
        {
            "name": "x6",
            "type": "range",
            "bounds": [0.0, 1.0],
        },
    ],
    experiment_name="test",
    objective_name="hartmann6",
    evaluation_function=hartmann_evaluation_function,
    minimize=True,  # Optional, defaults to False.
    parameter_constraints=["x1 + x2 <= 20"],  # Optional.
    outcome_constraints=["l2norm <= 1.25"],  # Optional.
    total_trials=30,  # Optional.
)
```

<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x2. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x3. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x4. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x5. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter x6. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='x1', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x2', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x3', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x4', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x5', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='x6', parameter_type=FLOAT, range=[0.0, 1.0])], parameter_constraints=[ParameterConstraint(1.0*x1 + 1.0*x2 <= 20.0)]).`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.modelbridge.dispatch_utils: verbose, disable_progbar, and jit_compile are not yet supported when using choose_generation_strategy with ModularBoTorchModel, dropping these arguments.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 12 trials, BoTorch for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.managed_loop: Started full optimization with 30 steps.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 1...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 2...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 3...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 4...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 5...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 6...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 7...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 8...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 9...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 10...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.
[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 11...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 12...`
}
</CellOutput>


<CellOutput>
{
  `/home/runner/work/Ax/Ax/ax/modelbridge/cross_validation.py:439: UserWarning:
Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:13] ax.service.managed_loop: Running optimization trial 13...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:25] ax.service.managed_loop: Running optimization trial 14...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:34] ax.service.managed_loop: Running optimization trial 15...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:30:45] ax.service.managed_loop: Running optimization trial 16...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:31:01] ax.service.managed_loop: Running optimization trial 17...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:31:13] ax.service.managed_loop: Running optimization trial 18...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:31:25] ax.service.managed_loop: Running optimization trial 19...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:31:41] ax.service.managed_loop: Running optimization trial 20...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:31:59] ax.service.managed_loop: Running optimization trial 21...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:32:10] ax.service.managed_loop: Running optimization trial 22...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:32:23] ax.service.managed_loop: Running optimization trial 23...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:32:32] ax.service.managed_loop: Running optimization trial 24...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:32:41] ax.service.managed_loop: Running optimization trial 25...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:32:48] ax.service.managed_loop: Running optimization trial 26...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:32:57] ax.service.managed_loop: Running optimization trial 27...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:33:04] ax.service.managed_loop: Running optimization trial 28...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:33:10] ax.service.managed_loop: Running optimization trial 29...`
}
</CellOutput>


<CellOutput>
{
  `[INFO 02-03 18:33:17] ax.service.managed_loop: Running optimization trial 30...`
}
</CellOutput>

And we can introspect optimization results:

```python
best_parameters
```

<CellOutput>
{
  `{'x1': 0.23567978731096606,
 'x2': 0.0,
 'x3': 0.3977677811660017,
 'x4': 0.23318947211468297,
 'x5': 0.2891941142712672,
 'x6': 0.5978973574601063}`
}
</CellOutput>

```python
means, covariances = values
means
```

<CellOutput>
{
  `{'hartmann6': -2.863688596444597, 'l2norm': 0.8422013805006452}`
}
</CellOutput>

For comparison, minimum of Hartmann6 is:

```python
hartmann6.fmin
```

<CellOutput>
{
  `-3.32237`
}
</CellOutput>

## 3. Plot results

Here we arbitrarily select "x1" and "x2" as the two parameters to plot for both metrics,
"hartmann6" and "l2norm".

```python
render(plot_contour(model=model, param_x="x1", param_y="x2", metric_name="hartmann6"))
```

<PlotlyFigure data={require('./assets/plot_data/f4ffdc55-db5d-4dab-8ed1-62bc4cc4b733.json')} />

```python
render(plot_contour(model=model, param_x="x1", param_y="x2", metric_name="l2norm"))
```

<PlotlyFigure data={require('./assets/plot_data/1e5a12a3-a5a6-4b74-913a-47a6e6fcea39.json')} />

We also plot optimization trace, which shows best hartmann6 objective value seen by each
iteration of the optimization:

```python
# `plot_single_method` expects a 2-d array of means, because it expects to average means from multiple
# optimization runs, so we wrap out best objectives array in another array.
best_objectives = np.array(
    [[trial.objective_mean for trial in experiment.trials.values()]]
)
best_objective_plot = optimization_trace_single_method(
    y=np.minimum.accumulate(best_objectives, axis=1),
    optimum=hartmann6.fmin,
    title="Model performance vs. # of iterations",
    ylabel="Hartmann6",
)
render(best_objective_plot)
```

<PlotlyFigure data={require('./assets/plot_data/84fb1bef-5fd5-465d-9bbc-650e1e25bb7e.json')} />

